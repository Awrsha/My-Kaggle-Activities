# -*- coding: utf-8 -*-
"""Energy_Demand_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Oda-tUxFrPKbajGZqmIYT_lKe2a--H9D
"""

# Importing necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import statsmodels.api as sm

# Set pandas display options to show all columns
pd.set_option('display.max_columns', None)

# Suppressing warnings
warnings.filterwarnings("ignore")

# Modeling and evaluation
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import RandomizedSearchCV, train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import Lasso, Ridge
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor
from xgboost import XGBRegressor
from catboost import CatBoostRegressor
import plotly.graph_objects as go

# Load dataset from Google Drive
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

"""---

## Data Science Project Lifecycle:

1. **Business Understanding:**  
   This initial stage revolves around defining the business goal and understanding the problem to be solved.

2. **Data Collection and Understanding:**  
   In this stage, all relevant data sources are identified, and the data is explored to understand its structure and relevance.

3. **Data Preparation:**  
   Data preparation involves cleaning, integrating, treating missing values, handling outliers, and formatting data for analysis.

4. **Exploratory Data Analysis:**  
   This step includes gaining insights into the data through visualization and understanding the factors that may affect the model.

5. **Feature Engineering:**  
   Feature engineering involves selecting, deriving new features, and preparing the data for modeling.

6. **Modeling:**  
   The modeling phase is where predictive models are built using machine learning algorithms to solve the business problem.

7. **Model Evaluation:**  
   After building models, they are rigorously evaluated to select the best-performing model for deployment.

8. **Model Deployment:**  
   The final step is deploying the selected model in a production environment to make predictions or recommendations.

Each stage in the data science lifecycle is critical, and proper execution of these steps ensures the success of a data science project.

## Business Understanding

The main objective of using the dataset for the project "The Hourly Energy Equation: Balancing Supply and Demand in Real-Time" is to develop a model that can accurately forecast hourly energy consumption and generation in Spain, balancing supply and demand in real-time. This forecasting model will be used to optimize the energy infrastructure and ensure a reliable and efficient energy supply.

---

### Dataset Overview

This project revolves around a comprehensive dataset spanning four years, capturing crucial insights into Spain's electrical ecosystem. The data amalgamates hourly records of electrical consumption and generation, settlement prices, and pertinent weather information for the five largest cities in Spain. Sourced from ENTSOE (Transmission Service Operator data) and REE (Red Electric España - Spanish TSO), this dataset not only provides a rich repository of historical information but also includes forecasts by the Transmission Service Operator (TSO) for both consumption and pricing.

The dataset is unique because it contains hourly data for electrical consumption and the respective forecasts by the TSO for consumption and pricing. This allows for a more accurate and detailed analysis of energy demand and supply patterns, which is essential for developing a model that can balance supply and demand in real-time.

The dataset is publicly available via ENTSOE and REE and may be found in the links provided. The inspiration behind using this dataset is the potential for deep learning and machine learning techniques to improve energy forecasting and contribute to the transition to a renewable-based electrical infrastructure.

---

### Problem Statement

1. The energy sector is undergoing a radical transformation, with the transition to renewable energy sources and the need to balance supply and demand in real-time becoming increasingly important.

2. The main challenges facing the power grid are the integration of bidirectional energy flows, the management of non-dispatchable generation, and the deployment of a digital telecommunications infrastructure that allows control and automation.

In the context of this project, the problem statement can be defined as:

**Problem Statement:**  
How can we develop a model that accurately forecasts hourly energy consumption and generation in Spain, balancing supply and demand in real-time, using the provided dataset of electrical consumption, generation, pricing, and weather data for Spain?

The objective is to create a model that can handle the changing demand for electricity and the use of different energy sources in today's fast-changing world, ensuring a reliable and efficient energy supply. This model will be used to optimize the energy infrastructure and maintain a high level of reliability in the power grid.

---

## References

- [ENTSOE (European Network of Transmission System Operators for Electricity)](https://transparency.entsoe.eu/dashboard/show)
- [REE (Red Eléctrica de España)](https://www.esios.ree.es/en/market-and-prices?date=11-03-2024)

## Data Collection and Understanding
"""

# Weather data
weather_df = pd.read_csv("/content/drive/MyDrive/DataSets/Electricity Demand Forecast/weather_data.csv")

# Electricity demand data
electricity_df = pd.read_csv("/content/drive/MyDrive/DataSets/Electricity Demand Forecast/electricity_data.csv")

# Displaying the first few rows of the electricity demand data
electricity_df.head()

electricity_df.columns

"""The dataset contains the following columns:

1. **DateTime**:  
   Represents the timestamp for each record, indicating the date and time of observation.

2. **Wind Offshore (MW)**:  
   Indicates the offshore wind electricity generation in megawatts (MW), offering an estimate of offshore wind electricity generation for the next day.

3. **Wind Onshore (MW)**:  
   Represents the onshore wind electricity generation in megawatts (MW), offering an estimate of onshore wind electricity generation for the next day.

4. **Total Load Forecast (MW)**:  
   Reflects the forecasted electrical demand, providing an estimate of the total electricity demand for a specific period.

5. **Total Load Actual (MW)**:  
   Quantifies the actual electrical demand, indicating the real-time total electricity demand for a specific period.

6. **Price Day Ahead (EUR/MWh)**:  
   Represents the forecasted electricity price in euros per megawatt-hour (EUR/MWh) for a specific period.

7. **Price Actual (EUR/MWh)**:  
   Indicates the actual electricity price in euros per megawatt-hour (EUR/MWh) for a specific period.

These columns collectively provide a comprehensive snapshot of Spain's electrical generation, consumption, pricing, and renewable energy forecast data, allowing for in-depth analysis and forecasting.
"""

# Displaying the first few rows of the weather demand data
weather_df.head()

weather_df.columns

"""### Data Understanding - Short Explanation of Columns:

1. **DateTime and Location:**
   - **dt_iso**: Datetime index localized to Central European Time (CET).
   - **city_name**: Name of the city.

2. **Temperature and Pressure:**
   - **temp**: Temperature in Kelvin.
   - **temp_min**: Minimum temperature in Kelvin.
   - **temp_max**: Maximum temperature in Kelvin.
   - **pressure**: Atmospheric pressure in hectopascals (hPa).

3. **Humidity and Wind:**
   - **humidity**: Humidity in percentage.
   - **wind_speed**: Wind speed in meters per second (m/s).
   - **wind_deg**: Wind direction.

4. **Precipitation and Snow:**
   - **rain_1h**: Rain in the last hour in millimeters (mm).
   - **rain_3h**: Rain in the last 3 hours in millimeters (mm).
   - **snow_3h**: Snow in the last 3 hours in millimeters (mm).

5. **Cloud Cover and Weather Description:**
   - **clouds_all**: Cloud cover in percentage.
   - **weather_id**: Code used to describe weather.
   - **weather_main**: Short description of current weather.
   - **weather_description**: Long description of current weather.
   - **weather_icon**: Weather icon code for the website.

These columns collectively provide comprehensive weather-related information, including temperature, pressure, humidity, wind speed, precipitation, cloud cover, and weather descriptions, facilitating detailed analysis and forecasting.

Data Exploration and Preparation
"""

electricity_df.info()

weather_df.info()

# Converting datetime columns to datetime objects with UTC timezone
weather_df['dt_iso'] = pd.to_datetime(weather_df['dt_iso'], errors='coerce', utc=True)
electricity_df['time'] = pd.to_datetime(electricity_df['time'], errors='coerce', utc=True)

# Merging the two dataframes on the common columns 'time' and 'dt_iso'
merged_df = pd.merge(electricity_df, weather_df, left_on='time', right_on='dt_iso', how='inner')

# If the common column is no longer needed, you can drop it
merged_df = merged_df.drop(['dt_iso'], axis=1)

# Displaying the first few rows of the merged dataframe
merged_df.head()

merged_df.columns

# Counting duplicate entries based on the 'time' column
duplicate_count = merged_df.duplicated(subset=['time']).sum()

# Printing the count of duplicates
print("Number of duplicates based on the time column:", duplicate_count)

# Displaying information about the merged dataframe
merged_df.info()

# Dropping duplicate entries based on the 'time' column
merged_df = merged_df.drop_duplicates(subset=['time'])

# Displaying the first few rows of the updated dataframe
merged_df.head()

merged_df.info()

merged_df.describe()

merged_df.columns

# Correcting the typo in the selected columns list
selected_columns = [
    'time', 'generation biomass', 'generation fossil brown coal/lignite',
    'generation fossil gas', 'generation fossil hard coal', 'generation fossil oil',
    'generation hydro pumped storage consumption', 'generation hydro run-of-river and poundage',
    'generation hydro water reservoir', 'generation nuclear', 'generation other',
    'generation other renewable', 'generation solar', 'generation waste', 'generation wind onshore',
    'total load actual', 'price actual', 'temp_min', 'temp_max', 'pressure', 'humidity',
    'wind_speed', 'clouds_all', 'weather_main'
]

# Creating a new dataframe with specified columns
merged_new = merged_df.loc[:, selected_columns]

# Displaying the first few rows of the new dataframe
merged_new.head()

# Renaming the column 'generation wind onshore' to 'generation wind'
merged_new.rename(columns={"generation wind onsh": "generation wind"}, inplace=True)

# Checking for missing values in the dataframe
missing_values = merged_new.isnull().sum()

# Printing the count of missing values for each column
print("Count of missing values in each column:")
print(missing_values)

# Remove rows with null values from the DataFrame
merged_new = merged_new.dropna()

# Check if there are any null values in the DataFrame
null_check = merged_new.isnull().values.any()

# Printing True if there are null values, False if there are no null values
print("Are there any null values in the DataFrame?", null_check)

# Converting the 'time' column to datetime objects with UTC timezone
merged_new['time'] = pd.to_datetime(merged_new['time'], errors='coerce', utc=True)

# Displaying information about the updated dataframe
merged_new.info()

merged_new.describe()

merged_new.columns

# Extracting date and hour components
merged_new['date'] = merged_new['time'].dt.date
merged_new['hour'] = merged_new['time'].dt.hour

# Extracting month, week, and day components
merged_new['month'] = merged_new['time'].dt.month
merged_new['week'] = merged_new['time'].dt.isocalendar().week  # ISO week number
merged_new['day'] = merged_new['time'].dt.day
merged_new['year'] = merged_new['time'].dt.year

# Dropping the 'time' column
merged_new = merged_new.drop(columns=['time'])

# Reordering the columns
column_order = ['date', 'year', 'month', 'week', 'day', 'hour'] + [col for col in merged_new.columns if col not in ['date', 'year', 'month', 'week', 'day', 'hour']]
merged_new = merged_new[column_order]

# Displaying the first few rows of the updated dataframe
merged_new.head()

"""Exploratory Data Analysis"""

# Create a copy of the original DataFrame
df_eda = merged_new.copy()

# Perform your EDA on the copied DataFrame (df_copy)
# For example, you can check the first few rows of the copied DataFrame
df_eda.head()

# Set pandas display options to show all columns
pd.set_option('display.max_columns', None)

# Display the first 5 rows of the DataFrame with all columns visible
df_eda.head()

df_eda.describe()

# Save the DataFrame to a CSV file without including the index
df_eda.to_csv('df_eda.csv', index=False)

# Print the column names of the DataFrame
print("Column names of the DataFrame:")
print(df_eda.columns)

# Assuming df_eda is your DataFrame
# Extracting unique weather categories from the 'weather_main' column
unique_weather_categories = df_eda['weather_main'].unique()

# Printing unique weather categories
print("Unique Weather Categories:")
print(unique_weather_categories)

# Set the figure size
plt.figure(figsize=(10, 6))

# Create a countplot for weather categories
ax = sns.countplot(x='weather_main', data=df_eda, palette='viridis')

# Set title and labels
plt.title('Frequency Distribution of Weather Categories')
plt.xlabel('Weather Category')
plt.ylabel('Count')

# Rotate x-axis labels for better readability
plt.xticks(rotation=45)

# Add count labels on top of each bar
for p in ax.patches:
    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='baseline', fontsize=10, color='black')

# Show the plot
plt.tight_layout()
plt.show()

# Replace weather categories
df_eda['weather_main'] = df_eda['weather_main'].replace({
    'mist': 'clouds',
    'thunderstorm': 'rain',
    'drizzle': 'rain',
    'fog': 'clouds',
    'smoke': 'clear',
    'haze': 'clouds'
})

# Create the countplot
plt.figure(figsize=(10, 6))
ax = sns.countplot(x='weather_main', data=df_eda, palette='viridis')

# Set title and labels
plt.title('Frequency Distribution of Weather Categories')
plt.xlabel('Weather Category')
plt.ylabel('Count')
plt.xticks(rotation=45)

# Add count labels on top of each bar
for p in ax.patches:
    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='baseline', fontsize=10, color='black')

# Show the plot
plt.tight_layout()
plt.show()

# Set the figure size
plt.figure(figsize=(12, 8))

# Create a boxplot to visualize the impact of weather on total load
sns.boxplot(x='weather_main', y='total load actual', data=df_eda, palette='viridis')

# Set title and labels
plt.title('Impact of Weather on Total Load')
plt.xlabel('Weather Category')
plt.ylabel('Total Load Actual')

# Rotate x-axis labels for better readability
plt.xticks(rotation=45)

# Show the plot
plt.tight_layout()
plt.show()

# Set the figure size
plt.figure(figsize=(12, 8))

# Create a boxplot to visualize the impact of weather on electricity price
sns.boxplot(x='weather_main', y='price actual', data=df_eda, palette='viridis')

# Set title and labels
plt.title('Impact of Weather on Electricity Price')
plt.xlabel('Weather Category')
plt.ylabel('Price Actual')

# Rotate x-axis labels for better readability
plt.xticks(rotation=45)

# Show the plot
plt.tight_layout()
plt.show()

# Set the figure size
plt.figure(figsize=(12, 8))

# Create a countplot to visualize weather variation over months
sns.countplot(x='year', hue='weather_main', data=df_eda, palette='viridis')

# Set title and labels
plt.title('Weather Variation Over Years')
plt.xlabel('Year')
plt.ylabel('Count')

# Add a legend with title for weather categories
plt.legend(title='Weather Category')

# Show the plot
plt.tight_layout()
plt.show()

# List of groups based on variables
groups = [
    ['year', 'month', 'week', 'day', 'hour'],
    ['generation biomass', 'generation fossil brown coal/lignite', 'generation fossil gas',
     'generation fossil hard coal', 'generation fossil oil', 'generation hydro pumped storage consumption',
     'generation hydro run-of-river and poundage', 'generation hydro water reservoir', 'generation nuclear',
     'generation other', 'generation other renewable', 'generation solar', 'generation waste'],
    ['total load actual', 'price actual', 'temp_min', 'temp_max', 'pressure', 'humidity', 'wind_speed', 'clouds_all']
]

# Plot heatmap for each group
for group in groups:
    plt.figure(figsize=(12, 8))
    try:
        sns.heatmap(df_eda[group].corr(), annot=True, cmap='coolwarm', center=0)
        plt.title('Correlation Heatmap for ' + ', '.join(group))
        plt.show()
    except KeyError as e:
        print(f"KeyError: {e} not found in DataFrame. Skipping this group.")

df_eda.columns

import matplotlib.pyplot as plt
import seaborn as sns

# Check the actual column names in the DataFrame
print(df_eda.columns)

# Adjust the selected_columns list based on the actual column names
selected_columns = ['generation biomass', 'generation fossil brown coal/lignite', 'generation fossil gas',
                    'generation fossil hard coal', 'generation fossil oil',
                    'generation hydro pumped storage consumption',
                    'generation hydro run-of-river and poundage', 'generation hydro water reservoir',
                    'generation nuclear', 'generation other', 'generation other renewable', 'generation solar',
                    'generation waste', 'total load actual', 'price actual']

# Set the number of columns and rows for the subplot
num_cols = 4
num_rows = (len(selected_columns) - 1) // num_cols + 1

# Create a subplot
fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 20))
fig.subplots_adjust(hspace=0.5)

# Flatten the axes array to handle the case when there's only one row or column
axes = axes.flatten()

# Plot histograms for each selected column
for i, col in enumerate(selected_columns):
    sns.histplot(df_eda[col], bins=20, kde=True, ax=axes[i])
    axes[i].set_title(col, fontsize=12)
    axes[i].set_xlabel(None)
    axes[i].set_ylabel('Frequency', fontsize=10)

# Hide any empty subplots if the number of selected columns is not a multiple of num_cols
for j in range(len(selected_columns), num_rows * num_cols):
    fig.delaxes(axes[j])

# Show the plot
plt.tight_layout()
plt.show()

# Define the list of weather columns
weather_columns = ['temp_min', 'temp_max', 'pressure', 'humidity']

# Set up the matplotlib figure with a 2x2 subplot layout
plt.figure(figsize=(15, 10))

# Loop through each weather variable and create histograms
for i, column in enumerate(weather_columns, 1):
    plt.subplot(2, 2, i)
    sns.histplot(df_eda[column], bins=20, kde=True, color='skyblue', edgecolor='black')
    plt.title(f'Histogram of {column}')
    plt.xlabel(column)
    plt.ylabel('Frequency')

# Adjust layout and show the plot
plt.tight_layout()
plt.show()

df_eda.head()

df_eda.to_csv('EDA_pb.csv', index=False)
df_eda.columns

# Selecting all numerical columns
numerical_columns = ['year', 'month', 'week', 'day', 'hour', 'generation biomass',
                     'generation fossil brown coal/lignite', 'generation fossil hard coal',
                     'generation fossil oil', 'generation hydro pumped storage consumption',
                     'generation hydro run-of-river and poundage', 'generation hydro water reservoir',
                     'generation nuclear', 'generation other', 'generation other renewable',
                     'generation solar', 'generation waste', 'total load actual',
                     'price actual']

# Create a correlation matrix
correlation_matrix = df_eda[numerical_columns].corr()

# Set up the matplotlib figure
plt.figure(figsize=(12, 10))

# Create a heatmap of the correlation matrix
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)

# Set title and adjust layout
plt.title('Correlation Matrix of Numerical Columns', fontsize=16)
plt.tight_layout()

# Show the plot
plt.show()

df_eda.info()

"""Feature Engineering

"""

# List of generation columns
generation_columns = ['generation biomass', 'generation fossil brown coal/lignite', 'generation fossil gas',
                      'generation fossil hard coal', 'generation fossil oil',
                      'generation hydro pumped storage consumption',
                      'generation hydro run-of-river and poundage', 'generation hydro water reservoir',
                      'generation nuclear', 'generation other', 'generation other renewable', 'generation solar',
                      'generation waste']

# Add a new column 'total load generated' by summing up all generation columns
df_eda['total load generated'] = df_eda[generation_columns].sum(axis=1)

# Display the DataFrame with the new column
df_eda.head()

# Create a new column 'demand_supply_difference'
df_eda['demand_supply_difference'] = df_eda['total load generated'] - df_eda['total load actual']
df_eda[['date', 'hour', 'demand_supply_difference']].head()

# Create a new column 'shifted_demand_supply' by shifting 'demand_supply_difference' column by one row
df_eda['shifted_demand_supply'] = df_eda['demand_supply_difference'].shift(-1)

# Select and display the relevant columns: 'date', 'hour', and 'shifted_demand_supply'
shifted_demand_supply_df = df_eda[['date', 'hour', 'shifted_demand_supply']].head()

# Display the DataFrame
shifted_demand_supply_df

# Assuming you want to remove the last row since while building model it will cause issue and error
df_eda = df_eda.drop(df_eda.index[-1])
df_eda.columns

df_eda.shape

"""Model Building"""

# Assuming df_eda is your DataFrame
X = df_eda[['month', 'week', 'day', 'hour',
            'generation biomass', 'generation fossil brown coal/lignite', 'generation fossil gas',
            'generation fossil hard coal', 'generation fossil oil',
            'generation hydro pumped storage consumption',
            'generation hydro run-of-river and poundage', 'generation hydro water reservoir',
            'generation nuclear', 'generation other', 'generation other renewable', 'generation solar',
            'generation waste', 'temp_min', 'temp_max', 'pressure', 'humidity',
            'wind_speed', 'clouds_all', 'weather_main', 'total load actual', 'price actual']]
y = df_eda["shifted_demand_supply"]

# Define numerical and categorical features
num_features = X.select_dtypes(exclude="object").columns
cat_features = X.select_dtypes(include="object").columns

# Create transformers for numerical and categorical features
numeric_transformer = StandardScaler()
oh_transformer = OneHotEncoder()

# Create preprocessor pipeline
preprocessor = ColumnTransformer(
    transformers=[
        ("OneHotEncoder", oh_transformer, cat_features),
        ("StandardScaler", numeric_transformer, num_features)
    ])

# Apply preprocessing pipeline to X
X_preprocessed = preprocessor.fit_transform(X)

# Split the preprocessed data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42)

# Display the shapes of training and testing sets
X_train.shape, X_test.shape

def evaluate_model(true, predicted):
    """Function to evaluate model performance."""
    mae = mean_absolute_error(true, predicted)
    mse = mean_squared_error(true, predicted)
    rmse = np.sqrt(mse)
    r2_square = r2_score(true, predicted)
    return mae, rmse, r2_square

# Define models to be evaluated
models = {
    "Linear Regression": LinearRegression(),
    "Lasso": Lasso(),
    "Ridge": Ridge(),
    "K-Neighbors Regressor": KNeighborsRegressor(),
    "Decision Tree": DecisionTreeRegressor(),
    "Random Forest Regressor": RandomForestRegressor(),
    "XGBRegressor": XGBRegressor(),
    "CatBoosting Regressor": CatBoostRegressor(verbose=False),
    "AdaBoost Regressor": AdaBoostRegressor()
}

# Initialize an empty list to store results
results = []

# Loop through each model
for model_name, model in models.items():
    # Fit the model on training data
    model.fit(X_train, y_train)

    # Make predictions
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)

    # Evaluate Train and Test dataset
    model_train_mae, model_train_rmse, model_train_r2 = evaluate_model(y_train, y_train_pred)
    model_test_mae, model_test_rmse, model_test_r2 = evaluate_model(y_test, y_test_pred)

    # Append results to list
    results.append({
        'Model': model_name,
        'Train RMSE': model_train_rmse,
        'Train MAE': model_train_mae,
        'Train R2': model_train_r2,
        'Test RMSE': model_test_rmse,
        'Test MAE': model_test_mae,
        'Test R2': model_test_r2
    })

# Convert the list of dictionaries to a DataFrame
results_df = pd.DataFrame(results)

# Display the results DataFrame
results_df.head(10)

# Selecting features (X) and target variable (y)
features = ['month', 'week', 'day', 'hour',
            'generation biomass', 'generation fossil brown coal/lignite', 'generation fossil gas',
            'generation fossil hard coal', 'generation fossil oil',
            'generation hydro pumped storage consumption',
            'generation hydro run-of-river and poundage', 'generation hydro water reservoir',
            'generation nuclear', 'generation other', 'generation other renewable', 'generation solar',
            'generation waste', 'temp_min', 'temp_max', 'pressure', 'humidity',
            'wind_speed', 'clouds_all', 'weather_main', 'total load actual', 'price actual']
target = 'shifted_demand_supply'

# Separate features and target variable
X = df_eda[features]
y = df_eda[target]

# Identify numerical and categorical features
numerical_features = X.select_dtypes(exclude="object").columns
categorical_features = X.select_dtypes(include="object").columns

# Preprocessing using ColumnTransformer
numeric_transformer = StandardScaler()
categorical_transformer = OneHotEncoder()
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# Transform features
X_transformed = preprocessor.fit_transform(X)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.2, random_state=42)

# Build and train the CatBoost Regressor model
catboost_model = CatBoostRegressor(verbose=False)
catboost_model.fit(X_train, y_train)

# Make predictions
y_pred = catboost_model.predict(X_test)

"""Model Evaluation"""

# Evaluate the model
try:
    # Calculate the difference between actual and predicted values
    y_diff = y_test - y_pred

    # Calculate MAE for positive and negative differences separately
    positive_mae = mean_absolute_error(y_test[y_diff >= 0], y_pred[y_diff >= 0])
    negative_mae = mean_absolute_error(y_test[y_diff < 0], y_pred[y_diff < 0])

    # Calculate overall MAE and R-squared
    overall_mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    # Print the results
    print(f"MAE for positive values: {positive_mae}")
    print(f"MAE for negative values: {negative_mae}")
    print(f"Overall MAE: {overall_mae}")
    print(f"R-squared (R2): {r2}")

except Exception as e:
    print(f"An error occurred during evaluation: {e}")

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, callbacks

# Data Preparation
# Ensure the data is properly preprocessed and scaled

# Model Architecture
model = keras.Sequential([
    layers.Dense(256, activation='relu', input_shape=(X_train.shape[1],)),
    layers.BatchNormalization(),
    layers.Dropout(0.5),
    layers.Dense(128, activation='relu'),
    layers.BatchNormalization(),
    layers.Dropout(0.5),
    layers.Dense(64, activation='relu'),
    layers.BatchNormalization(),
    layers.Dropout(0.5),
    layers.Dense(1)  # Output layer for regression
])

# Compile the model
optimizer = keras.optimizers.Adam(learning_rate=0.001)
model.compile(optimizer=optimizer, loss='mean_squared_error')

# Define callbacks
early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
learning_rate_scheduler = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)

# Train the model
history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stopping, learning_rate_scheduler], verbose=1)

# Evaluate the model
y_pred = model.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f"Mean Absolute Error (MAE): {mae}")
print(f"R-squared (R2): {r2}")

# Plot training and validation loss
plt.figure(figsize=(10, 6))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Plot predicted versus actual values
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, alpha=0.5)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], '--', color='red')
plt.title('Predicted vs Actual')
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.show()