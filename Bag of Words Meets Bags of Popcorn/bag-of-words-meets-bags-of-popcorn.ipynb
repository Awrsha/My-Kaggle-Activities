{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3971,"databundleVersionId":32703,"sourceType":"competition"},{"sourceId":7595466,"sourceType":"datasetVersion","datasetId":4421070}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-09T21:50:13.881163Z","iopub.execute_input":"2024-02-09T21:50:13.881503Z","iopub.status.idle":"2024-02-09T21:50:14.254456Z","shell.execute_reply.started":"2024-02-09T21:50:13.881474Z","shell.execute_reply":"2024-02-09T21:50:14.253551Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/word2vec-nlp-tutorial/testData.tsv.zip\n/kaggle/input/word2vec-nlp-tutorial/sampleSubmission.csv\n/kaggle/input/word2vec-nlp-tutorial/unlabeledTrainData.tsv.zip\n/kaggle/input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip\n/kaggle/input/imdb-master-csv-zip/imdb_master.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"# Suppressing warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Importing necessary libraries\nimport numpy as np\nimport pandas as pd\nimport re\nimport os\nimport html\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom gensim.models import Word2Vec, Phrases\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Dropout, Bidirectional\nfrom tensorflow.keras.layers import Embedding\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\nfrom xgboost import XGBClassifier","metadata":{"execution":{"iopub.status.busy":"2024-02-09T21:50:14.256823Z","iopub.execute_input":"2024-02-09T21:50:14.257812Z","iopub.status.idle":"2024-02-09T21:50:28.192747Z","shell.execute_reply.started":"2024-02-09T21:50:14.257774Z","shell.execute_reply":"2024-02-09T21:50:28.191867Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-02-09 21:50:25.311782: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-09 21:50:25.311836: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-09 21:50:25.313367: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# Initializing classifiers\nrandom_forest = RandomForestClassifier()\ngradient_boosting = GradientBoostingClassifier()\ndecision_tree = DecisionTreeClassifier()\nlogistic_regression = LogisticRegression()\nk_neighbors = KNeighborsClassifier()\ngaussian_naive_bayes = GaussianNB()\nbernoulli_naive_bayes = BernoulliNB()\nxgboost_classifier = XGBClassifier()\nmultinomial_naive_bayes = MultinomialNB()","metadata":{"execution":{"iopub.status.busy":"2024-02-09T21:50:28.193964Z","iopub.execute_input":"2024-02-09T21:50:28.195502Z","iopub.status.idle":"2024-02-09T21:50:28.201192Z","shell.execute_reply.started":"2024-02-09T21:50:28.195471Z","shell.execute_reply":"2024-02-09T21:50:28.200196Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Define file paths\ntrain_data_path = \"../input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip\"\ntest_data_path = \"../input/word2vec-nlp-tutorial/testData.tsv.zip\"\nadditional_data_csv_path = \"../input/imdb-master-csv-zip/imdb_master.csv\"\n\n# Reading datasets\ntrain = pd.read_csv(train_data_path, sep='\\t')\ntest = pd.read_csv(test_data_path, sep='\\t')\n\n# Reading additional CSV file\natt = pd.read_csv(additional_data_csv_path, encoding='latin')","metadata":{"execution":{"iopub.status.busy":"2024-02-09T21:50:28.202434Z","iopub.execute_input":"2024-02-09T21:50:28.202730Z","iopub.status.idle":"2024-02-09T21:50:30.761584Z","shell.execute_reply.started":"2024-02-09T21:50:28.202701Z","shell.execute_reply":"2024-02-09T21:50:30.760730Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Selecting relevant columns and renaming them\natt = att[[\"review\", \"label\"]]\natt.columns = [\"review\", \"sentiment\"]\n\n# Removing rows with \"unsup\" sentiment and converting sentiment to binary\natt = att[att.sentiment != \"unsup\"]\natt[\"sentiment\"] = att[\"sentiment\"].map({\"neg\": 0, \"pos\": 1})\n\n# Displaying the first few rows\natt.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-09T21:50:30.764850Z","iopub.execute_input":"2024-02-09T21:50:30.765603Z","iopub.status.idle":"2024-02-09T21:50:30.821478Z","shell.execute_reply.started":"2024-02-09T21:50:30.765564Z","shell.execute_reply":"2024-02-09T21:50:30.820444Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                              review  sentiment\n0  Once again Mr. Costner has dragged out a movie...          0\n1  This is an example of why the majority of acti...          0\n2  First of all I hate those moronic rappers, who...          0\n3  Not even the Beatles could write songs everyon...          0\n4  Brass pictures (movies is not a fitting word f...          0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Once again Mr. Costner has dragged out a movie...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>This is an example of why the majority of acti...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>First of all I hate those moronic rappers, who...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Not even the Beatles could write songs everyon...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Brass pictures (movies is not a fitting word f...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Getting the shapes of the datasets\ntrain_shape = train.shape\ntest_shape = test.shape\natt_shape = att.shape\n\n# Displaying the shapes\ntrain_shape, test_shape, att_shape","metadata":{"execution":{"iopub.status.busy":"2024-02-09T21:50:30.822834Z","iopub.execute_input":"2024-02-09T21:50:30.823176Z","iopub.status.idle":"2024-02-09T21:50:30.830552Z","shell.execute_reply.started":"2024-02-09T21:50:30.823119Z","shell.execute_reply":"2024-02-09T21:50:30.829652Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"((25000, 3), (25000, 2), (50000, 2))"},"metadata":{}}]},{"cell_type":"code","source":"# Concatenating the datasets\ndf = pd.concat([train, att, test], ignore_index=True)\n\n# Displaying information about the combined DataFrame\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2024-02-09T21:50:30.831975Z","iopub.execute_input":"2024-02-09T21:50:30.832283Z","iopub.status.idle":"2024-02-09T21:50:30.872007Z","shell.execute_reply.started":"2024-02-09T21:50:30.832249Z","shell.execute_reply":"2024-02-09T21:50:30.870988Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 100000 entries, 0 to 99999\nData columns (total 3 columns):\n #   Column     Non-Null Count   Dtype  \n---  ------     --------------   -----  \n 0   id         50000 non-null   object \n 1   sentiment  75000 non-null   float64\n 2   review     100000 non-null  object \ndtypes: float64(1), object(2)\nmemory usage: 2.3+ MB\n","output_type":"stream"}]},{"cell_type":"code","source":"# Displaying the first few rows of the combined DataFrame\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-09T21:50:30.873173Z","iopub.execute_input":"2024-02-09T21:50:30.873437Z","iopub.status.idle":"2024-02-09T21:50:30.884411Z","shell.execute_reply.started":"2024-02-09T21:50:30.873414Z","shell.execute_reply":"2024-02-09T21:50:30.883394Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"       id  sentiment                                             review\n0  5814_8        1.0  With all this stuff going down at the moment w...\n1  2381_9        1.0  \\The Classic War of the Worlds\\\" by Timothy Hi...\n2  7759_3        0.0  The film starts with a manager (Nicholas Bell)...\n3  3630_4        0.0  It must be assumed that those who praised this...\n4  9495_8        1.0  Superbly trashy and wondrously unpretentious 8...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>sentiment</th>\n      <th>review</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5814_8</td>\n      <td>1.0</td>\n      <td>With all this stuff going down at the moment w...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2381_9</td>\n      <td>1.0</td>\n      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7759_3</td>\n      <td>0.0</td>\n      <td>The film starts with a manager (Nicholas Bell)...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3630_4</td>\n      <td>0.0</td>\n      <td>It must be assumed that those who praised this...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9495_8</td>\n      <td>1.0</td>\n      <td>Superbly trashy and wondrously unpretentious 8...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.review=html.unescape(df.review)\n# removing html symbols\n\ndf.review=df.review.str.replace('http\\S+|www.\\S+', '', case=False).str.replace(r\"\\&\\#[0-9]+\\;\",\"\", regex=True) \\\n.str.replace(r\"[^\\w\\s]\",\"\").str.replace(\"\\d+\",\"\").str.replace(r\"[\\s]+\",\" \",regex=True) \\\n.str.replace(\"\\n\",\" \").replace(\"\\r\",\"\").str.replace(\"_\",\" \").str.lower()","metadata":{"execution":{"iopub.status.busy":"2024-02-09T21:50:30.885812Z","iopub.execute_input":"2024-02-09T21:50:30.886108Z","iopub.status.idle":"2024-02-09T21:50:43.778006Z","shell.execute_reply.started":"2024-02-09T21:50:30.886083Z","shell.execute_reply":"2024-02-09T21:50:43.776959Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def remove_emoji(text):\n    # Define the emoji pattern using Unicode ranges\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    # Remove emojis from the text using the pattern\n    clean_text = emoji_pattern.sub(r'', text)\n    return clean_text","metadata":{"execution":{"iopub.status.busy":"2024-02-09T21:50:43.779453Z","iopub.execute_input":"2024-02-09T21:50:43.779826Z","iopub.status.idle":"2024-02-09T21:50:43.785230Z","shell.execute_reply.started":"2024-02-09T21:50:43.779787Z","shell.execute_reply":"2024-02-09T21:50:43.784177Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from unicodedata import normalize\n\ndef remove_accent(text):\n    # Normalize the text to decompose and separate accents\n    normalized_text = normalize(\"NFKD\", text)\n    # Encode the normalized text to ASCII, ignoring any characters that cannot be represented in ASCII\n    ascii_text = normalized_text.encode(\"ascii\", \"ignore\")\n    # Decode the ASCII text back to UTF-8, ignoring any decoding errors\n    clean_text = ascii_text.decode(\"utf-8\", \"ignore\")\n    return clean_text\n\n# Apply the remove_accent function to the 'review' column of the DataFrame\ndf[\"review\"] = df[\"review\"].apply(remove_accent)","metadata":{"execution":{"iopub.status.busy":"2024-02-09T21:50:43.786203Z","iopub.execute_input":"2024-02-09T21:50:43.786482Z","iopub.status.idle":"2024-02-09T21:50:44.588832Z","shell.execute_reply.started":"2024-02-09T21:50:43.786458Z","shell.execute_reply":"2024-02-09T21:50:44.587676Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Display the first few rows of the DataFrame\nprint(\"First few rows of the DataFrame:\")\nprint(df.head())\n\n# Display the value counts of the 'sentiment' column\nprint(\"\\nValue counts of the 'sentiment' column:\")\nprint(df['sentiment'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2024-02-09T21:50:44.590523Z","iopub.execute_input":"2024-02-09T21:50:44.590922Z","iopub.status.idle":"2024-02-09T21:50:44.624214Z","shell.execute_reply.started":"2024-02-09T21:50:44.590892Z","shell.execute_reply":"2024-02-09T21:50:44.623260Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"First few rows of the DataFrame:\n       id  sentiment                                             review\n0  5814_8        1.0  with all this stuff going down at the moment w...\n1  2381_9        1.0  \\the classic war of the worlds\\\" by timothy hi...\n2  7759_3        0.0  the film starts with a manager (nicholas bell)...\n3  3630_4        0.0  it must be assumed that those who praised this...\n4  9495_8        1.0  superbly trashy and wondrously unpretentious 8...\n\nValue counts of the 'sentiment' column:\nsentiment\n1.0    37500\n0.0    37500\nName: count, dtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"corpora = df[\"review\"].values\ntokenized = [word_tokenize(corpus) for corpus in corpora]\n\nprint(tokenized[2222])","metadata":{"execution":{"iopub.status.busy":"2024-02-09T21:50:44.625373Z","iopub.execute_input":"2024-02-09T21:50:44.625641Z","iopub.status.idle":"2024-02-09T21:55:29.441095Z","shell.execute_reply.started":"2024-02-09T21:50:44.625616Z","shell.execute_reply":"2024-02-09T21:55:29.440014Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"['go', 'immediately', 'and', 'rent', 'this', 'movie', '.', 'it', 'will', 'be', 'be', 'on', 'a', 'bottom', 'shelf', 'in', 'your', 'local', 'video', 'store', 'and', 'will', 'be', 'covered', 'in', 'dust', '.', 'no', 'one', 'will', 'have', 'touched', 'it', 'in', 'years', '.', 'it', 'may', 'even', 'be', 'a', '$', '.50', 'special', '!', 'it', \"'s\", 'worth', 'ten', 'bucks', ',', 'i', 'swear', '!', 'buy', 'it', '!', 'there', 'are', \"n't\", 'very', 'many', 'films', 'than', 'can', 'compare', 'with', 'this', '-', 'the', 'celluloid', 'version', 'of', 'that', 'goo', 'that', 'forms', 'at', 'the', 'bottom', 'of', 'a', 'trash', 'can', 'after', 'a', 'few', 'years', '.', 'yes', ',', 'i', 'gave', 'it', 'a', \"'1\", ',', \"'\", 'but', 'it', 'really', 'deserves', 'much', 'lower', '.', '1-10', 'scales', 'were', 'not', 'designed', 'with', 'stuff', 'like', 'this', 'in', 'mind', '.']\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenized = [list(filter(lambda x: len(x) > 1, document)) \\\n             for document in tokenized]\n\nprint(tokenized[2222])","metadata":{"execution":{"iopub.status.busy":"2024-02-09T21:55:29.445401Z","iopub.execute_input":"2024-02-09T21:55:29.445709Z","iopub.status.idle":"2024-02-09T21:55:34.879470Z","shell.execute_reply.started":"2024-02-09T21:55:29.445677Z","shell.execute_reply":"2024-02-09T21:55:34.878408Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"['go', 'immediately', 'and', 'rent', 'this', 'movie', 'it', 'will', 'be', 'be', 'on', 'bottom', 'shelf', 'in', 'your', 'local', 'video', 'store', 'and', 'will', 'be', 'covered', 'in', 'dust', 'no', 'one', 'will', 'have', 'touched', 'it', 'in', 'years', 'it', 'may', 'even', 'be', '.50', 'special', 'it', \"'s\", 'worth', 'ten', 'bucks', 'swear', 'buy', 'it', 'there', 'are', \"n't\", 'very', 'many', 'films', 'than', 'can', 'compare', 'with', 'this', 'the', 'celluloid', 'version', 'of', 'that', 'goo', 'that', 'forms', 'at', 'the', 'bottom', 'of', 'trash', 'can', 'after', 'few', 'years', 'yes', 'gave', 'it', \"'1\", 'but', 'it', 'really', 'deserves', 'much', 'lower', '1-10', 'scales', 'were', 'not', 'designed', 'with', 'stuff', 'like', 'this', 'in', 'mind']\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\nfrom gensim.models import word2vec\nimport numpy as np\n\n# Set numpy print options to suppress scientific notation\nnp.set_printoptions(suppress=True)\n\n# Define parameters for Word2Vec model\nfeature_size = 256\ncontext_size = 5\nmin_word = 1\nepochs = 50\nrandom_seed = 42\n\n# Train Word2Vec model\nword_vec_model = word2vec.Word2Vec(tokenized, \n                                   vector_size=feature_size, \n                                   window=context_size, \n                                   min_count=min_word, \n                                   epochs=epochs, \n                                   seed=random_seed)\n\n# Print the time taken for training\nprint(\"Word2Vec model training time:\")","metadata":{"execution":{"iopub.status.busy":"2024-02-09T21:55:34.880742Z","iopub.execute_input":"2024-02-09T21:55:34.881056Z","iopub.status.idle":"2024-02-09T22:17:18.764021Z","shell.execute_reply.started":"2024-02-09T21:55:34.881029Z","shell.execute_reply":"2024-02-09T22:17:18.763000Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Word2Vec model training time:\nCPU times: user 1h 55s, sys: 13.9 s, total: 1h 1min 8s\nWall time: 21min 43s\n","output_type":"stream"}]},{"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\n\n# Initialize the Tokenizer\ntokenizer = Tokenizer()\n\n# Fit the tokenizer on the tokenized text\ntokenizer.fit_on_texts(tokenized)\n\n# Convert the tokenized text to sequences of integers\nsequences = tokenizer.texts_to_sequences(tokenized)\n\n# Padding sequences to ensure uniform length\npadded_sequences = pad_sequences(sequences, padding='post')\n\n# Converting the padded sequences into a NumPy array\ntokenized_array = np.array(padded_sequences)\n\n# Checking the shape of the tokenized array\nprint(\"Shape of tokenized array:\", tokenized_array.shape)","metadata":{"execution":{"iopub.status.busy":"2024-02-09T22:19:42.049320Z","iopub.execute_input":"2024-02-09T22:19:42.050121Z","iopub.status.idle":"2024-02-09T22:20:14.493568Z","shell.execute_reply.started":"2024-02-09T22:19:42.050089Z","shell.execute_reply":"2024-02-09T22:20:14.492544Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Shape of tokenized array: (100000, 2419)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Creating a document vector by taking the mean of word vectors in each document\nmodel_array = np.array([word_vec_model.wv[doc].mean(axis=0) for doc in tokenized_array])\n\n# Creating DataFrame with document vectors and sentiment labels\nmodel_df = pd.DataFrame(model_array)\nmodel_df[\"sentiment\"] = df[\"sentiment\"]\n\n# Displaying the first few rows of the DataFrame\nprint(\"First few rows of the DataFrame:\")\nprint(model_df.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the DataFrame into training and testing sets\ndf_train = model_df[:75000]\ndf_test = model_df[75000:]\n\n# Confirm the sizes of the training and testing sets\nprint(\"Size of training set:\", len(df_train))\nprint(\"Size of testing set:\", len(df_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract the 'sentiment' column as the target variable for the training set\ny = df_train.pop(\"sentiment\")\n\n# Extract the remaining columns as the features for the training set\ntext = df_train\n\n# Remove the 'sentiment' column from the testing set\ntest = df_test.drop(columns=[\"sentiment\"])\n\n# Display the shapes of the training features, target variable, and testing features\nprint(\"Shapes of training features, target variable, and testing features:\")\nprint(\"Training features shape:\", text.shape)\nprint(\"Target variable shape:\", y.shape)\nprint(\"Testing features shape:\", test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit the model using the training features and target variable\nr.fit(text, y)\n\n# Make predictions on the testing set\npred = r.predict(test)\n\n# Convert the predictions to integers\npred = pred.astype(int)\n\n# Display the predictions\nprint(\"Predicted sentiments:\")\nprint(pred)","metadata":{"execution":{"iopub.status.busy":"2024-02-09T22:17:20.363191Z","iopub.status.idle":"2024-02-09T22:17:20.363639Z","shell.execute_reply.started":"2024-02-09T22:17:20.363402Z","shell.execute_reply":"2024-02-09T22:17:20.363420Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a new DataFrame for submission\nsubmission_df = pd.DataFrame()\n\n# Extract 'id' column from the testing set\nsubmission_df[\"id\"] = df.iloc[75000:][\"id\"]\n\n# Add predicted sentiments to the submission DataFrame\nsubmission_df[\"sentiment\"] = pred\n\n# Display the submission DataFrame\nprint(\"Submission DataFrame:\")\nprint(submission_df)\n\n# Save the submission DataFrame to a CSV file\nsubmission_df.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-02-09T22:17:20.364915Z","iopub.status.idle":"2024-02-09T22:17:20.365383Z","shell.execute_reply.started":"2024-02-09T22:17:20.365144Z","shell.execute_reply":"2024-02-09T22:17:20.365164Z"},"trusted":true},"execution_count":null,"outputs":[]}]}