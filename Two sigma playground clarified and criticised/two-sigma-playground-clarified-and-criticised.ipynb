{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import kagglegym\nimport numpy as np\nimport pandas as pd\nimport math\nimport time\nimport gc\nfrom sklearn.base import BaseEstimator, clone\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nclass FastBaggingRegressor(BaseEstimator):\n    def __init__(self, base_estimator=None, n_estimators=10, random_state=123456,\n                 max_samples=1.0, max_features=0, bootstrap=True, bootstrap_features=False, filter_estimators=False):\n        self.base_estimator = base_estimator\n        self.n_estimators = n_estimators\n        self.max_samples = max_samples\n        self.max_features = max_features\n        self.bootstrap = bootstrap\n        self.bootstrap_features = bootstrap_features\n        self.filter_estimators = filter_estimators\n        self.is_fitted = False\n        self.model_coef = np.zeros((n_estimators,))  # np.array(n_estimators, 1)\n        self.model_constants = np.zeros((n_estimators,))  # np.array(n_estimators, 1)\n        self.model_rewards = np.zeros((n_estimators,))  # np.array(n_estimators, 1)\n        self.n_features = None\n        self.prng = np.random.RandomState(random_state)\n\n    def normalized_rms_error(self, y_true, y_pred):\n        return np.sum((y_true - y_pred)**2) / np.sum((y_true - np.mean(y_true))**2)\n\n    def fit(self, X, y):\n        try:\n            X = X.values\n            y = y.values\n        except AttributeError:\n            pass\n\n        n_samples, self.n_features = X.shape\n        self.model_coef = np.zeros((self.n_estimators, self.n_features))\n\n        assert self.base_estimator is not None, 'please set base_estimator'\n\n        features_indices_ = np.arange(self.n_features)\n        sample_indices_ = np.arange(n_samples)\n        train_filter = np.zeros((n_samples,), dtype=bool)\n        current_model = clone(self.base_estimator)\n\n        for idx_ in range(self.n_estimators):\n            train_filter[:] = False\n            train_indices = self.prng.choice(sample_indices_, size=int(self.max_samples * n_samples), replace=True)\n            train_filter[train_indices] = True\n\n            if self.bootstrap_features:\n                n_selected_features = int(self.max_features)\n                current_features = self.prng.choice(features_indices_, size=n_selected_features, replace=False)\n            else:\n                current_features = features_indices_\n\n            current_model.fit(X[train_filter, current_features], y[train_filter])\n            y_pred = current_model.predict(X[~train_filter, current_features])\n\n            self.model_rewards[idx_] = self.normalized_rms_error(y[~train_filter], y_pred)\n            self.model_constants[idx_] = current_model.intercept_\n            self.model_coef[idx_, current_features] = current_model.coef_\n\n            if idx_ % 50 == 0:\n                print('FastBaggingRegressor finished %d out of %d' % (idx_, self.n_estimators))\n\n        self.final_model = clone(self.base_estimator)\n        if self.filter_estimators:\n            selected_models = self.model_rewards.argsort()[-int(0.5 * self.n_estimators):]\n            self.final_model.coef_ = np.mean(self.model_coef[selected_models, :], axis=0)\n            self.final_model.intercept_ = np.mean(self.model_constants[selected_models], axis=0)\n        else:\n            self.final_model.coef_ = np.mean(self.model_coef, axis=0)\n            self.final_model.intercept_ = np.mean(self.model_constants, axis=0)\n\n        self.is_fitted = True\n        self.coef_ = self.final_model.coef_\n        self.intercept_ = self.final_model.intercept_\n        return self\n\n    def predict(self, X):\n        if not self.is_fitted:\n            raise RuntimeError('Model is not fitted yet')\n        try:\n            X = X.values\n        except AttributeError:\n            pass\n        return self.final_model.predict(X)\n\n\ndef _reward(y_true, y_fit):\n    R2 = 1 - np.sum((y_true - y_fit)**2) / np.sum((y_true - np.mean(y_true))**2)\n    R = np.sign(R2) * math.sqrt(abs(R2))\n    return R\n\n\ndef index_chunker(data_df, n_chunks):\n    unique_timestamp = data_df[\"timestamp\"].unique()\n    n_total = len(unique_timestamp)\n    chunk_size = int(n_total / n_chunks)\n    chunk_boundaries = np.arange(0, n_total, chunk_size)\n    index_df = pd.DataFrame(data=True, index=data_df.index, columns=range(chunk_boundaries.shape[0]))\n\n    for idx_, pos in enumerate(chunk_boundaries):\n        if (pos + 2 * chunk_size) > n_total:\n            current_filter = (data_df.timestamp >= pos)\n            index_df.loc[current_filter, idx_] = False\n\n            if (idx_ + 1) <= index_df.shape[0]:\n                index_df = index_df.iloc[:, :(idx_ + 1)]\n            break\n\n        current_filter = (data_df.timestamp >= pos) & (data_df.timestamp < (pos + chunk_size))\n        index_df.loc[current_filter, idx_] = False\n\n    return index_df\n\n\ndef train_base_model_cv(train, y_train, model_columns, sk_model, predictor_name):\n    if predictor_name not in train.columns:\n        train.assign(**{predictor_name: np.nan})\n\n    train_columns = ['timestamp'] + model_columns\n    X_train = train[train_columns]\n\n    n_chunks = 10\n    train_test_df = index_chunker(X_train, n_chunks)\n\n    for column_idx_ in train_test_df.columns:\n        train_indices = train_test_df[column_idx_]\n        sk_model.fit(X_train.loc[train_indices, model_columns], y_train[train_indices])\n        train.loc[~train_indices, predictor_name] = sk_model.predict(X_train.loc[~train_indices, model_columns])\n\n    X_train = train[model_columns]\n    sk_model.fit(X_train, y_train)\n\n    return sk_model\n\n\ndef process_features_training(train, id_mean_columns_list, id_mean_rolling_list):\n    print('Processing features during training')\n\n    train['tech23'] = train['technical_20'] + train['technical_13'] - train['technical_30']\n    train['tech23_v2'] = train['technical_20'] - train['technical_30']\n\n    grouped_data_id = train.groupby('id')\n\n    process_tech23 = lambda x: (x - 0.925 * x.shift(1)) / 0.075\n    approx_y = grouped_data_id['tech23'].transform(process_tech23)\n    train['approx_y_prev'] = approx_y.fillna(0).clip(-0.07, 0.07)\n\n    approx_y = grouped_data_id['tech23_v2'].transform(process_tech23)\n    train['approx_y_prev_v2'] = approx_y.fillna(0).clip(-0.07, 0.07)\n\n    for current_column in id_mean_columns_list:\n        for n_rolling_length in id_mean_rolling_list:\n            temp_data = grouped_data_id[current_column].rolling(window=n_rolling_length).mean().fillna(0.0)\n            temp_data = pd.DataFrame(temp_data.values, index=temp_data.index.get_level_values(1))\n\n            new_column_name1 = f\"{current_column}_id_mean_{n_rolling_length}\"\n            train[new_column_name1] = temp_data\n\n            new_column_name2 = f\"{current_column}_id_diff_mean_{n_rolling_length}\"\n            train[new_column_name2] = train[current_column] - train[new_column_name1]\n\n    grouped_data_ts = train.groupby('timestamp')\n    average_values = grouped_data_ts['tech23'].agg([np.mean, np.std])\n    average_values_extended = average_values.loc[train['timestamp']].reset_index(drop=True)\n    train['tech23_cs_mean'] = average_values_extended['mean'].values\n    train['tech23_cs_std'] = average_values_extended['std'].values\n\n    n_rolling_length_long = 10\n    average_values = grouped_data_ts['tech23'].mean().rolling(n_rolling_length_long).mean().fillna(0.0)\n    train['tech23_cs_mean_10'] = average_values.loc[train['timestamp']].reset_index(drop=True).values\n\n    train_median = train.median(axis=0)\n    train = train.replace([np.inf, -np.inf], np.nan).fillna(train_median)\n\n    return train, train_median\n\n\ndef process_features_online(test, id_mean_columns_list, id_mean_rolling_list, prev_test_lists, train_median):\n    print('Processing features during prediction')\n\n    max_rolling_length = np.max(id_mean_rolling_list)\n\n    test['tech23'] = test['technical_20'] + test['technical_13'] - test['technical_30']\n    test['tech23_cs_mean'] = test['tech23'].mean(axis=0)\n    test['tech23_cs_std'] = test['tech23'].std(axis=0)\n\n    test_prev = prev_test_lists[-1]\n    approx_y_prev_values = test[['id', 'tech23']].set_index('id') - 0.925 * (\n                test_prev[['id', 'tech23']].set_index('id'))\n    approx_y_prev_values /= 0.075\n\n    test['approx_y_prev'] = approx_y_prev_values.loc[test.id].values\n    test['approx_y_prev'].fillna(0, inplace=True)\n\n    test['tech23_v2'] = test['technical_20'] - test['technical_30']\n    approx_y_prev_values = test[['id', 'tech23_v2']].set_index('id') - 0.925 * (\n                test_prev[['id', 'tech23_v2']].set_index('id'))\n    approx_y_prev_values /= 0.075\n\n    test['approx_y_prev_v2'] = approx_y_prev_values.loc[test.id].values\n    test['approx_y_prev_v2'].fillna(0, inplace=True)\n\n    prev_test_lists.append(test)\n    if len(prev_test_lists) > max_rolling_length:\n        prev_test_lists = prev_test_lists[1:]\n\n    test_temp = test.set_index(test.id)\n    for n_rolling_length in id_mean_rolling_list:\n        test_grouped_data_id = pd.concat(prev_test_lists[-n_rolling_length:], axis=0).groupby('id')\n        for current_column in id_mean_columns_list:\n            if current_column == 'approx_y_prev':\n                temp_data = test_grouped_data_id[current_column].median()\n            else:\n                temp_data = test_grouped_data_id[current_column].mean()\n\n            new_column_name1 = f\"{current_column}_id_mean_{n_rolling_length}\"\n            test_temp[new_column_name1] = temp_data\n\n            new_column_name2 = f\"{current_column}_id_diff_mean_{n_rolling_length}\"\n            test_temp[new_column_name2] = test_temp[current_column] - test_temp[new_column_name1]\n\n    test_grouped_data_id = pd.concat(prev_test_lists, axis=0).groupby('id')\n    test_temp['tech23_cs_mean_10'] = test_grouped_data_id['tech23_cs_mean'].mean()\n\n    test = test_temp.reset_index(drop=True)\n    test = test.fillna(train_median)\n\n    return test, prev_test_lists\n\n\nenv = kagglegym.make()\nobservation = env.reset()\n\ntrain = observation.train\n\nid_mean_columns_list = ['tech23', 'approx_y_prev', 'tech23_v2', 'approx_y_prev_v2']\nid_mean_rolling_list = [3, 5, 7, 10]\ntrain, train_median = process_features_training(train, id_mean_columns_list, id_mean_rolling_list)\n\nlow_y_cut = -0.085\nhigh_y_cut = 0.092\n\nmean_normalization_column = 'tech23_cs_mean_10'\nvolatility_column = 'tech23_cs_std'\ncs_factor = 0.25\n\ny_train = train['y'] - cs_factor * train[mean_normalization_column]\ny_train /= train[volatility_column]\n\nbase_models = []\nsimple_model = make_pipeline(StandardScaler(), LinearRegression(fit_intercept=True))\n\nbase_models_columns = [['tech23', 'tech23_id_mean_3'],\n                       ['technical_40', 'fundamental_11'],\n                       ['approx_y_prev', 'approx_y_prev_id_mean_3', 'approx_y_prev_id_mean_10'],\n                       # ... (add more base models as needed)\n                       ]\n\nn_base_models = len(base_models_columns)\n\nfor idx_ in range(n_base_models):\n    model_columns_ = base_models_columns[idx_]\n    predictor_name = f'pred_{idx_}'\n    sk_model_ = clone(simple_model)\n    sk_model_ = train_base_model_cv(train, y_train, model_columns_, sk_model_, predictor_name)\n    base_models.append(sk_model_)\n\nbase_models_outputs = [f'pred_{idx_}' for idx_ in range(n_base_models)]\nmodel_final_columns = base_models_outputs\n\nX_train = train[model_final_columns]\npredictor_stacked_name = 'stacked_pred'\n\nsk_model_final = FastBaggingRegressor(LinearRegression(fit_intercept=True), n_estimators=3000,\n                                      random_state=565776, max_samples=1.0, max_features=3,\n                                      bootstrap=True, bootstrap_features=True, filter_estimators=True)\n\nsk_model_final.fit(X_train, y_train)\n\nprint(f'sk_model_final with parameters intercept_ and coef_: {sk_model_final.intercept_}, {sk_model_final.coef_}')\n\nmax_rolling_length = np.max(id_mean_rolling_list)\nprev_test_lists = list(train.groupby('timestamp'))[-max_rolling_length:]\nprev_test_lists = [x[1] for x in prev_test_lists]\n\ni = 0\nreward_ = []\n\nwhile True:\n    test = observation.features\n    pred = observation.target\n\n    test, prev_test_lists = process_features_online(test, id_mean_columns_list, id_mean_rolling_list,\n                                                    prev_test_lists, train_median)\n\n    for temp_pred_name in model_final_columns + [predictor_stacked_name]:\n        if temp_pred_name not in test.columns:\n            test[temp_pred_name] = np.nan\n\n    for idx_local_ in range(n_base_models):\n        model_columns_ = base_models_columns[idx_local_]\n        predictor_name = f'pred_{idx_local_}'\n        sk_model_ = base_models[idx_local_]\n        X_current = test[model_columns_]\n        test[predictor_name] = sk_model_.predict(X_current)\n\n    X_current = test[model_final_columns]\n    test[predictor_stacked_name] = sk_model_final.predict(X_current)\n\n    test[predictor_stacked_name].replace([np.inf, -np.inf, np.nan], 0.0, inplace=True)\n\n    test['y_pred'] = test[predictor_stacked_name]\n    test['y_pred'] *= test[volatility_column]\n    test['y_pred'] += cs_factor * test[mean_normalization_column]\n\n    test['y_pred'] = test['y_pred'].clip(low_y_cut, high_y_cut)\n\n    pred['y'] = test['y_pred']\n    observation, reward, done, info = env.step(pred[['id', 'y']])\n    reward_.append(reward)\n\n    if i % 100 == 0:\n        print(reward, np.mean(np.array(reward_)))\n        collected = gc.collect()\n        print(f'Garbage collector: collected {collected} objects.')\n\n    i += 1\n\n    if done:\n        print(f'Finished ... {info[\"public_score\"]}')\n        break","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]}]}